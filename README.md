# ğŸš— Vehicle Insurance MLOps Pipeline

> **A complete, productionâ€‘grade MLOps project** that demonstrates how machine learning systems are designed, built, deployed, and maintained in real industry environments.

---

## ğŸ“Œ What This Project Demonstrates

This project showcases:

* âœ… **Industryâ€‘style project structure** (not notebookâ€‘only ML)
* âœ… **Endâ€‘toâ€‘end MLOps pipeline** from raw data to live deployment
* âœ… **Clean separation of concerns** using configuration, entity, components, and pipeline layers
* âœ… **Cloudâ€‘native ML** using AWS & MongoDB
* âœ… **CI/CD automation** using Docker, GitHub Actions, ECR, and EC2


##  Highâ€‘Level Architecture

```
MongoDB Atlas
      â†“
Data Ingestion
      â†“
Data Validation
      â†“
Data Transformation
      â†“
Model Trainer
      â†“
Model Evaluation
      â†“
Model Registry (AWS S3)
      â†“
Model Pusher
      â†“
FastAPI Application (Docker)
      â†“
AWS EC2 (CI/CD via GitHub Actions)
```

Each block above is implemented as an **independent, testable Python module**.

---

## ğŸ› ï¸ Technology Stack (Why These Tools?)

### Programming & ML

* **Python 3.10** â€“ stable, productionâ€‘friendly
* **Pandas / NumPy** â€“ data processing
* **Scikitâ€‘learn** â€“ classical ML modeling
* **FastAPI** â€“ lightweight, fast ML serving

### Data & Storage

* **MongoDB Atlas** â€“ realistic cloud NoSQL data source
* **AWS S3** â€“ model registry & artifact storage

### MLOps & Engineering

* Modular pipelines
* Artifactâ€‘based tracking
* Schemaâ€‘driven data validation
* Custom logging & exception handling

### DevOps & Cloud

* **Docker** â€“ environment consistency
* **AWS ECR** â€“ container registry
* **AWS EC2** â€“ production server
* **GitHub Actions** â€“ CI/CD automation

---

## ğŸ“ Detailed Project Structure

```
vehicle-insurance-mlops/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/          # Core ML pipeline logic
â”‚   â”œâ”€â”€ configuration/       # MongoDB & AWS connection logic
â”‚   â”œâ”€â”€ constants/           # Centralized constants & env keys
â”‚   â”œâ”€â”€ entity/              # Config & artifact definitions
â”‚   â”œâ”€â”€ exception/           # Custom exception handling
â”‚   â”œâ”€â”€ logger/              # Central logging system
â”‚   â”œâ”€â”€ utils/               # Reusable helper functions
â”‚
â”œâ”€â”€ notebooks/               # EDA & MongoDB upload demos
â”œâ”€â”€ templates/               # FastAPI HTML templates
â”œâ”€â”€ static/                  # Static assets
â”œâ”€â”€ artifact/                # Generated pipeline artifacts (ignored in git)
â”œâ”€â”€ app.py                   # FastAPI application entry point
â”œâ”€â”€ demo.py                  # Training pipeline trigger
â”œâ”€â”€ Dockerfile               # Docker configuration
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ setup.py                 # Local package installation
â”œâ”€â”€ pyproject.toml           # Modern Python packaging
â””â”€â”€ .github/workflows/       # CI/CD pipelines
```

---

## âš™ï¸ STEPâ€‘BYâ€‘STEP IMPLEMENTATION GUIDE

---

## ğŸ”¹ STEP 1: Project Template Creation

A Python script (`template.py`) is used to generate a **standardized folder structure**.

```bash
python template.py
```

ğŸ”¹ **Why this matters**:
Consistent structure is critical in large ML systems to maintain readability and scalability.

---

## ğŸ”¹ STEP 2: Local Package Configuration

Files used:

* `setup.py`
* `pyproject.toml`

Purpose:

* Treat the project as a **Python package**
* Enable clean imports like:

  ```python
  from src.components.data_ingestion import DataIngestion
  ```

ğŸ“„ Reference: `crashcourse.txt`

---

## ğŸ”¹ STEP 3: Environment Setup

```bash
conda create -n vehicle python=3.10 -y
conda activate vehicle
pip install -r requirements.txt
```

Verify installation:

```bash
pip list
```

ğŸ”¹ Ensures reproducible development environment.

---

## ğŸ”¹ STEP 4: MongoDB Atlas Configuration

MongoDB is used as the **raw data source** to simulate real production data ingestion.

Steps:

1. Create MongoDB Atlas account
2. Deploy **M0 (free tier)** cluster
3. Create DB user
4. Allow network access: `0.0.0.0/0`
5. Copy Python connection string

Set environment variable:

**Bash**

```bash
export MONGODB_URL="mongodb+srv://<username>:<password>@..."
```

**PowerShell**

```powershell
$env:MONGODB_URL="mongodb+srv://<username>:<password>@..."
```

---

## ğŸ”¹ STEP 5: Logging & Exception Handling

Custom modules:

* `logger/`
* `exception/`

Purpose:

* Centralized logging across pipeline
* Meaningful error traces

Tested via `demo.py`.

---

## ğŸ”¹ STEP 6: Exploratory Data Analysis (EDA)

Notebooks included for:

* Data understanding
* Feature engineering logic
* Schema preparation

ğŸ”¹ **Note**: ML logic is NOT executed in notebooks.

---

## ğŸ”¹ STEP 7: Data Ingestion Component

Key responsibilities:

* Connect to MongoDB
* Fetch data in keyâ€‘value format
* Convert to Pandas DataFrame
* Save raw artifacts

Implemented using:

* `configuration.mongo_db_connections.py`
* `data_access/`
* `components.data_ingestion.py`

---

## ğŸ”¹ STEP 8: Data Validation

Driven by:

* `schema.yaml`

Checks include:

* Column presence
* Data types
* Missing values

Ensures **training data quality**.

---

## ğŸ”¹ STEP 9: Data Transformation

Includes:

* Feature encoding
* Scaling
* Train/test split

Reusable transformation objects are stored as artifacts.

---

## ğŸ”¹ STEP 10: Model Trainer

Responsibilities:

* Train ML model
* Save trained model
* Generate evaluation metrics

Designed to be easily replaceable with new models.

---

## ğŸ”¹ STEP 11: AWS & Model Registry Setup

Services used:

* IAM
* S3

Purpose:

* Store trained models centrally
* Enable versioning & rollback

---

## ğŸ”¹ STEP 12: Model Evaluation & Model Pusher

* Compare new model vs previous model
* Push to S3 if performance improves

Implements **productionâ€‘style gating logic**.

---

## ğŸ”¹ STEP 13: Prediction Pipeline & FastAPI

FastAPI endpoints:

* `/predict`
* `/training`

Supports:

* Realâ€‘time predictions
* Onâ€‘demand retraining

---

## ğŸ”¹ STEP 14: Dockerization

* Dockerfile
* .dockerignore

Purpose:

* Consistent runtime
* Easy deployment

---

## ğŸ”¹ STEP 15: CI/CD with GitHub Actions

Pipeline stages:

1. Build Docker image
2. Push to AWS ECR
3. Pull image on EC2
4. Run container automatically

Uses **selfâ€‘hosted EC2 runner**.

---

## ğŸ”¹ STEP 16: EC2 Deployment

* Ubuntu EC2
* Docker installed
* Port 5000 exposed

Access app:

```
http://<EC2_PUBLIC_IP>:5000
```
http://54.87.1.186:5000/

## ğŸ‘¤ Author

**Yasir Wali**
Aspiring MLOps 
Focused on building scalable ML systems

